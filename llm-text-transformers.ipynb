{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":6847931,"sourceType":"datasetVersion","datasetId":3936750},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256},{"sourceId":7018354,"sourceType":"datasetVersion","datasetId":4035516}],"dockerImageVersionId":30627,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-27T02:55:35.274833Z","iopub.execute_input":"2023-12-27T02:55:35.275171Z","iopub.status.idle":"2023-12-27T02:55:35.664818Z","shell.execute_reply.started":"2023-12-27T02:55:35.275142Z","shell.execute_reply":"2023-12-27T02:55:35.663747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n","metadata":{}},{"cell_type":"markdown","source":"Hello everyone. In this notebook I am going to use self-attention mechanism based transformers to do the AI generated text detection.","metadata":{}},{"cell_type":"markdown","source":"# Loading of Datasets","metadata":{}},{"cell_type":"markdown","source":"I am going to use the giant 400k dataset which is available on kaggle.For low computation purposes we are going to only use 90000 rows of it, and concatenate it with the orignal dataset.As the orignal dataset is very unbalanced (1375(0) and 3(1)) so we use equal 45000,45000 rows from the huge dataset for the input.If we pass the imbalanced dataset to the \nmodel ,it will get biased towards the majority class, which affect our model performance, also transformers works more well if we feed them with more data","metadata":{}},{"cell_type":"code","source":"train1=pd.read_csv('/kaggle/input/augmented-data-for-llm-detect-ai-generated-text/final_train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:13:01.058601Z","iopub.execute_input":"2023-12-26T11:13:01.059084Z","iopub.status.idle":"2023-12-26T11:13:19.345536Z","shell.execute_reply.started":"2023-12-26T11:13:01.059053Z","shell.execute_reply":"2023-12-26T11:13:19.344699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train1.shape ","metadata":{"execution":{"iopub.status.busy":"2023-12-26T06:54:51.632838Z","iopub.execute_input":"2023-12-26T06:54:51.633670Z","iopub.status.idle":"2023-12-26T06:54:51.642555Z","shell.execute_reply.started":"2023-12-26T06:54:51.633629Z","shell.execute_reply":"2023-12-26T06:54:51.641246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2=pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:13:32.778720Z","iopub.execute_input":"2023-12-26T11:13:32.779496Z","iopub.status.idle":"2023-12-26T11:13:32.866763Z","shell.execute_reply.started":"2023-12-26T11:13:32.779464Z","shell.execute_reply":"2023-12-26T11:13:32.865817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test1=pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:23:57.390754Z","iopub.execute_input":"2023-12-26T11:23:57.391661Z","iopub.status.idle":"2023-12-26T11:23:57.400644Z","shell.execute_reply.started":"2023-12-26T11:23:57.391625Z","shell.execute_reply":"2023-12-26T11:23:57.399626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test2=pd.read_csv('/kaggle/input/augmented-data-for-llm-detect-ai-generated-text/final_test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:13:46.515373Z","iopub.execute_input":"2023-12-26T11:13:46.516005Z","iopub.status.idle":"2023-12-26T11:13:50.528152Z","shell.execute_reply.started":"2023-12-26T11:13:46.515970Z","shell.execute_reply":"2023-12-26T11:13:50.527321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test2.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-26T06:55:11.826102Z","iopub.execute_input":"2023-12-26T06:55:11.826463Z","iopub.status.idle":"2023-12-26T06:55:11.832462Z","shell.execute_reply.started":"2023-12-26T06:55:11.826434Z","shell.execute_reply":"2023-12-26T06:55:11.831484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the giant dataset the target column is *label* , so I am going to create a new column *generated* and its values are copied from *label* column and then I drop the *label* column","metadata":{}},{"cell_type":"code","source":"train1['generated']=train1['label']","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:14:09.420741Z","iopub.execute_input":"2023-12-26T11:14:09.421423Z","iopub.status.idle":"2023-12-26T11:14:09.432196Z","shell.execute_reply.started":"2023-12-26T11:14:09.421385Z","shell.execute_reply":"2023-12-26T11:14:09.431116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train1.drop(['label'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:14:13.120245Z","iopub.execute_input":"2023-12-26T11:14:13.121186Z","iopub.status.idle":"2023-12-26T11:14:13.172156Z","shell.execute_reply.started":"2023-12-26T11:14:13.121140Z","shell.execute_reply":"2023-12-26T11:14:13.171138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\n# Assuming 'generated' is the column name in your 'train1' DataFrame\n# Replace 'train1' with the actual DataFrame variable\nfig = px.histogram(train1, x='generated', title='Distribution of Generated Column in the large dataset',\n                   labels={'generated': 'Generated Column', 'count': 'Count'},\n                   color='generated',  # Use the 'generated' column for color\n                   color_discrete_map={'0': 'yellow', '1': 'red'},  # Set colors for 0 and 1\n                   category_orders={'generated': ['0', '1']})\n                    # Display additional information on hover\n\n# Show the plot\nfig.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:14:19.199642Z","iopub.execute_input":"2023-12-26T11:14:19.200415Z","iopub.status.idle":"2023-12-26T11:14:21.705316Z","shell.execute_reply.started":"2023-12-26T11:14:19.200377Z","shell.execute_reply":"2023-12-26T11:14:21.704354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(train2, x='generated', title='Distribution of Generated Column in the orignal dataset',\n                   labels={'generated': 'Generated Column', 'count': 'Count'},\n                   color='generated',  # Use the 'generated' column for color\n                   color_discrete_map={'0': 'yellow', '1': 'red'},  # Set colors for 0 and 1\n                   category_orders={'generated': ['0', '1']})\n                    # Display additional information on hover\n\n# Show the plot\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-26T06:55:51.926695Z","iopub.execute_input":"2023-12-26T06:55:51.927060Z","iopub.status.idle":"2023-12-26T06:55:52.003081Z","shell.execute_reply.started":"2023-12-26T06:55:51.927033Z","shell.execute_reply":"2023-12-26T06:55:52.002134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I am droping the prompt_id ,id column for the model training.","metadata":{}},{"cell_type":"code","source":"train2.drop(['id','prompt_id'],axis=1,inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:14:27.968610Z","iopub.execute_input":"2023-12-26T11:14:27.969358Z","iopub.status.idle":"2023-12-26T11:14:27.974472Z","shell.execute_reply.started":"2023-12-26T11:14:27.969326Z","shell.execute_reply":"2023-12-26T11:14:27.973489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_df = train1\n\n# Separate AI-generated and human-generated texts\nai_generated_df = original_df[original_df['generated'] == 1].sample(n=45000, random_state=42)\nhuman_generated_df = original_df[original_df['generated'] == 0].sample(n=45000, random_state=42)\n\n# Concatenate the two dataframes\nsampled_df = pd.concat([ai_generated_df, human_generated_df])\n\n# Shuffle the dataset\nsampled_df = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:14:31.599813Z","iopub.execute_input":"2023-12-26T11:14:31.600154Z","iopub.status.idle":"2023-12-26T11:14:31.670149Z","shell.execute_reply.started":"2023-12-26T11:14:31.600127Z","shell.execute_reply":"2023-12-26T11:14:31.669343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled_df","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:14:38.566606Z","iopub.execute_input":"2023-12-26T11:14:38.567527Z","iopub.status.idle":"2023-12-26T11:14:38.585279Z","shell.execute_reply.started":"2023-12-26T11:14:38.567479Z","shell.execute_reply":"2023-12-26T11:14:38.584254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1=pd.concat([train2,sampled_df])","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:14:43.808626Z","iopub.execute_input":"2023-12-26T11:14:43.809045Z","iopub.status.idle":"2023-12-26T11:14:43.819104Z","shell.execute_reply.started":"2023-12-26T11:14:43.809014Z","shell.execute_reply":"2023-12-26T11:14:43.817848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.pie(df1, names='generated', title='Distribution of Generated Column',\n             labels={'generated': 'Generated Column'},\n             color_discrete_map={'0': 'yellow', '1': 'red'})\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:14:46.533915Z","iopub.execute_input":"2023-12-26T11:14:46.534611Z","iopub.status.idle":"2023-12-26T11:14:46.607944Z","shell.execute_reply.started":"2023-12-26T11:14:46.534578Z","shell.execute_reply":"2023-12-26T11:14:46.607014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it is balanced.","metadata":{}},{"cell_type":"code","source":"df1","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:14:54.267691Z","iopub.execute_input":"2023-12-26T11:14:54.268849Z","iopub.status.idle":"2023-12-26T11:14:54.280389Z","shell.execute_reply.started":"2023-12-26T11:14:54.268802Z","shell.execute_reply":"2023-12-26T11:14:54.279344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.drop_duplicates(inplace=True,ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:14:57.958530Z","iopub.execute_input":"2023-12-26T11:14:57.959360Z","iopub.status.idle":"2023-12-26T11:14:58.547859Z","shell.execute_reply.started":"2023-12-26T11:14:57.959326Z","shell.execute_reply":"2023-12-26T11:14:58.546531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.reset_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:15:01.768713Z","iopub.execute_input":"2023-12-26T11:15:01.769402Z","iopub.status.idle":"2023-12-26T11:15:01.774570Z","shell.execute_reply.started":"2023-12-26T11:15:01.769369Z","shell.execute_reply":"2023-12-26T11:15:01.773491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.drop(['index'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:15:05.288760Z","iopub.execute_input":"2023-12-26T11:15:05.289150Z","iopub.status.idle":"2023-12-26T11:15:05.302678Z","shell.execute_reply.started":"2023-12-26T11:15:05.289121Z","shell.execute_reply":"2023-12-26T11:15:05.301715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:15:10.498851Z","iopub.execute_input":"2023-12-26T11:15:10.499230Z","iopub.status.idle":"2023-12-26T11:15:10.511912Z","shell.execute_reply.started":"2023-12-26T11:15:10.499188Z","shell.execute_reply":"2023-12-26T11:15:10.510829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Preprocessing && Tokenization","metadata":{}},{"cell_type":"code","source":"import re # regular expression library\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('punkt')\ndef normalize(text):\n    # Replace with whitespace to separate 'ðŸ˜ƒ\\n\\nFor'\n    text = text.replace(r\"\\n\", r\" \")\n    text = text.replace(r\"\\r\", r\" \")\n    # Drop puntuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Remove extra spaces from 'ðŸ˜ƒ  For' to 'ðŸ˜ƒ For'\n    text = re.sub(r\"\\s+\", r\" \", text)\n    # Remove URLs\n    text = re.sub(r'http\\S+', '', text)\n    # Remove leading and trailing whitespace\n    text = text.strip()\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    #lower case the all capital alphabets\n    text=text.lower()\n    stop_words = set(stopwords.words('english'))\n    word_tokens=word_tokenize(text)\n    text = ' '.join([word for word in word_tokens if word not in stop_words])\n    return text\n\ndf1['text'] = df1['text'].apply(lambda x: normalize(x))\ntest1['text'] = test1['text'].apply(lambda x: normalize(x))","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:26:05.780767Z","iopub.execute_input":"2023-12-26T11:26:05.781185Z","iopub.status.idle":"2023-12-26T11:30:31.682702Z","shell.execute_reply.started":"2023-12-26T11:26:05.781154Z","shell.execute_reply":"2023-12-26T11:30:31.681700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\ntokenizer=Tokenizer()\ntokenizer.fit_on_texts(df1['text'])# gives a index on each word\nb1=len(tokenizer.word_index)# total no of words","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:31:56.497791Z","iopub.execute_input":"2023-12-26T11:31:56.498665Z","iopub.status.idle":"2023-12-26T11:32:23.881621Z","shell.execute_reply.started":"2023-12-26T11:31:56.498629Z","shell.execute_reply":"2023-12-26T11:32:23.880482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b1","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:32:40.898711Z","iopub.execute_input":"2023-12-26T11:32:40.899468Z","iopub.status.idle":"2023-12-26T11:32:40.905286Z","shell.execute_reply.started":"2023-12-26T11:32:40.899437Z","shell.execute_reply":"2023-12-26T11:32:40.904359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a=tokenizer.texts_to_sequences(df1['text'])\nmaxi=max(len(x) for x in a)# longest sentence size","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:33:16.927832Z","iopub.execute_input":"2023-12-26T11:33:16.928965Z","iopub.status.idle":"2023-12-26T11:33:27.550952Z","shell.execute_reply.started":"2023-12-26T11:33:16.928923Z","shell.execute_reply":"2023-12-26T11:33:27.550158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxi","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:34:31.608726Z","iopub.execute_input":"2023-12-26T11:34:31.609891Z","iopub.status.idle":"2023-12-26T11:34:31.616089Z","shell.execute_reply.started":"2023-12-26T11:34:31.609849Z","shell.execute_reply":"2023-12-26T11:34:31.615130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\na1=pad_sequences(a,maxlen=maxi,padding='post')","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:35:34.409681Z","iopub.execute_input":"2023-12-26T11:35:34.410512Z","iopub.status.idle":"2023-12-26T11:35:36.165438Z","shell.execute_reply.started":"2023-12-26T11:35:34.410479Z","shell.execute_reply":"2023-12-26T11:35:36.164637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1['text']=a1.tolist()","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:35:53.419642Z","iopub.execute_input":"2023-12-26T11:35:53.420511Z","iopub.status.idle":"2023-12-26T11:35:56.354180Z","shell.execute_reply.started":"2023-12-26T11:35:53.420476Z","shell.execute_reply":"2023-12-26T11:35:56.353262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nX = np.vstack(df1['text'].to_numpy())\ny=np.vstack(df1['generated'].to_numpy())","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:36:20.186706Z","iopub.execute_input":"2023-12-26T11:36:20.187538Z","iopub.status.idle":"2023-12-26T11:36:28.685664Z","shell.execute_reply.started":"2023-12-26T11:36:20.187504Z","shell.execute_reply":"2023-12-26T11:36:28.684661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:36:33.038946Z","iopub.execute_input":"2023-12-26T11:36:33.039303Z","iopub.status.idle":"2023-12-26T11:36:33.244540Z","shell.execute_reply.started":"2023-12-26T11:36:33.039270Z","shell.execute_reply":"2023-12-26T11:36:33.243736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:36:39.988030Z","iopub.execute_input":"2023-12-26T11:36:39.988711Z","iopub.status.idle":"2023-12-26T11:36:39.995033Z","shell.execute_reply.started":"2023-12-26T11:36:39.988675Z","shell.execute_reply":"2023-12-26T11:36:39.993928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model(Transformers)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Reshape,Embedding, Layer, Dense, Dropout, MultiHeadAttention, LayerNormalization, Input, GlobalAveragePooling1D,UpSampling1D\nfrom tensorflow.keras.layers import LSTM, Bidirectional\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\n\n\nclass TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, heads, neurons):\n        super(TransformerEncoder, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=heads, key_dim=embed_dim)\n        self.ffn = Sequential(\n            [layers.Dense(neurons, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(0.5)\n        self.dropout2 = layers.Dropout(0.5)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\nclass TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:40:15.499957Z","iopub.execute_input":"2023-12-26T11:40:15.500674Z","iopub.status.idle":"2023-12-26T11:40:15.516609Z","shell.execute_reply.started":"2023-12-26T11:40:15.500641Z","shell.execute_reply":"2023-12-26T11:40:15.515833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_dim = 10\nheads = 10\nneurons = 32\nmaxlen = maxi\nvocab_size = b1+1\n\ninputs = layers.Input(shape=(maxlen,))\nembedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\nx = embedding_layer(inputs)\ntransformer_block = TransformerEncoder(embed_dim, heads, neurons)\nx = transformer_block(x)\nx = layers.GlobalAveragePooling1D()(x)\nx = Dropout(0.20)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inputs, outputs=outputs)","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:40:24.491664Z","iopub.execute_input":"2023-12-26T11:40:24.492038Z","iopub.status.idle":"2023-12-26T11:40:29.009922Z","shell.execute_reply.started":"2023-12-26T11:40:24.492010Z","shell.execute_reply":"2023-12-26T11:40:29.008848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:40:40.763532Z","iopub.execute_input":"2023-12-26T11:40:40.764373Z","iopub.status.idle":"2023-12-26T11:40:40.784962Z","shell.execute_reply.started":"2023-12-26T11:40:40.764341Z","shell.execute_reply":"2023-12-26T11:40:40.783991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train,y_train,\n                    validation_split=0.2,\n                    epochs=5,\n                    batch_size=32,\n                    )","metadata":{"execution":{"iopub.status.busy":"2023-12-26T11:40:48.166915Z","iopub.execute_input":"2023-12-26T11:40:48.167599Z","iopub.status.idle":"2023-12-26T12:05:14.215623Z","shell.execute_reply.started":"2023-12-26T11:40:48.167568Z","shell.execute_reply":"2023-12-26T12:05:14.214364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","metadata":{"execution":{"iopub.status.busy":"2023-12-26T12:05:14.218138Z","iopub.execute_input":"2023-12-26T12:05:14.218488Z","iopub.status.idle":"2023-12-26T12:05:55.531849Z","shell.execute_reply.started":"2023-12-26T12:05:14.218459Z","shell.execute_reply":"2023-12-26T12:05:55.530861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Testing on the test part of the huge dataset of the model**","metadata":{}},{"cell_type":"code","source":"# test_sequences = tokenizer.texts_to_sequences(test2['text'])\n# X_test_padded = pad_sequences(test_sequences, maxlen=maxi)  # Adjust maxlen based on your model\n\n# Make predictions using the trained model\n# predictions = model.evaluate(X_test_padded,test2['label'],verbose=0)","metadata":{"execution":{"iopub.status.busy":"2023-12-26T12:07:47.288473Z","iopub.execute_input":"2023-12-26T12:07:47.289331Z","iopub.status.idle":"2023-12-26T12:10:31.849983Z","shell.execute_reply.started":"2023-12-26T12:07:47.289289Z","shell.execute_reply":"2023-12-26T12:10:31.849076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"Accuracy: %.2f%%\" % (predictions[1]*100))","metadata":{"execution":{"iopub.status.busy":"2023-12-26T12:11:16.817496Z","iopub.execute_input":"2023-12-26T12:11:16.818266Z","iopub.status.idle":"2023-12-26T12:11:16.823154Z","shell.execute_reply.started":"2023-12-26T12:11:16.818231Z","shell.execute_reply":"2023-12-26T12:11:16.822248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission File Creation","metadata":{}},{"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(test1['text'])\nX_test_padded = pad_sequences(test_sequences, maxlen=maxi)  # Adjust maxlen based on your model\n\n# Make predictions using the trained model\npredictions = model.predict(X_test_padded)\n\n# Assuming 'id' is the column containing unique identifiers in the test set\n# Create a DataFrame to store the results\nresults_df = pd.DataFrame({'id': test1['id'], 'generated': predictions.flatten()})\n\n# Save the results to a CSV file\nresults_df.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-26T12:12:00.187440Z","iopub.execute_input":"2023-12-26T12:12:00.187810Z","iopub.status.idle":"2023-12-26T12:12:01.675691Z","shell.execute_reply.started":"2023-12-26T12:12:00.187770Z","shell.execute_reply":"2023-12-26T12:12:01.674242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final=pd.read_csv('submission.csv')\nfinal.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-26T07:52:22.401522Z","iopub.execute_input":"2023-12-26T07:52:22.402224Z","iopub.status.idle":"2023-12-26T07:52:22.413794Z","shell.execute_reply.started":"2023-12-26T07:52:22.402191Z","shell.execute_reply":"2023-12-26T07:52:22.412613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Keypoints\n","metadata":{}},{"cell_type":"markdown","source":"The key and important insights from this competition which I get are below:\n* The dataset is imbalanced very much ,so we have to balance it by upsampling or using external dataset.In my case I have used a external dataset of kaggle\n* I have also seen that there are many typos and grammatical errors in the train dataset. So when you used external dataset which are correct and majority in class(90K vs 1378) then thw high and advanced LLMs overfits the training data ,you get a high level of accuracy . But when you used this model for the hidden test dataset you get 0.85 around LB score due to the typos available in the hidden test data ,infact due to this typos the LLMs are become unable to that wrong semantics of that data.\n* Obviously in this case Tf-Idf with simple classification model overkills this LLMs because they doesn't focus on the semantics of the text ,they just do the count / frequency of the word, so they get good LB as well as accuracy.\n* But in real life scenario definitely this high LB scored models don't get good results without focusing on the semantics ,definitely the LLMs traditionally overkills this simple models.\n ","metadata":{}}]}